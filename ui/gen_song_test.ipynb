{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchtext\n",
    "import torchdata\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import pretty_midi\n",
    "import fluidsynth\n",
    "import music21\n",
    "import soundfile\n",
    "import pyphen\n",
    "#TODO fix whatever this is\n",
    "#music21.environment.set(\"musescoreDirectPNGPath\", \"/usr/bin/musescore\") # tell music21 where MuseScore is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model%2 != 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:,0:-1]\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, unembedded_input_size, word_vocab_size, syll_vocab_size, embed_size = 10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.unembedded_input_size = unembedded_input_size\n",
    "        size_wo_lyrics = unembedded_input_size - 2\n",
    "        self.embedded_input_size = size_wo_lyrics + embed_size * 2\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, embed_size)\n",
    "        self.syll_embedding = nn.Embedding(syll_vocab_size, embed_size)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(self.embedded_input_size, dropout = 0) # for now, no dropout cause no trust\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model = self.embedded_input_size, \n",
    "            nhead = 2,\n",
    "            dim_feedforward=4,\n",
    "            dropout=0,\n",
    "            batch_first=True\n",
    "            )\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embedded_input_size)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers, num_layers=4, norm=self.norm)\n",
    "\n",
    "        # need linear layer to match input sizes for cross-attention in decoder\n",
    "        self.encoder_out = nn.Linear(self.embedded_input_size, 3) \n",
    "\n",
    "        decoder_layers = nn.TransformerDecoderLayer(\n",
    "            d_model = 3, \n",
    "            nhead = 1,\n",
    "            dim_feedforward=4,\n",
    "            dropout=0,\n",
    "            batch_first=True\n",
    "            )\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layers, num_layers=4)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src):\n",
    "        batch_size = src.shape[0]\n",
    "        seq_len = src.shape[1]\n",
    "\n",
    "        src_emb = self.embed_lyrics(src)\n",
    "        src_emb = self.pos_encoder(src_emb) \n",
    "\n",
    "        memory = self.encoder(src_emb)\n",
    "        memory = self.encoder_out(memory)\n",
    "\n",
    "        # generate a melody with same seq_len as src\n",
    "        tgt = torch.zeros((batch_size, 1, 3)) # start with zero note \n",
    "        for i in range(seq_len - 1): # -1 cause we already have 0 note\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.shape[1])\n",
    "            output = self.decoder(tgt, memory, tgt_mask)\n",
    "            output = output[:, -1] # extracts the inference because the output is formatted weirdly \n",
    "            output = output.view(batch_size, 1, -1) # reshapes output to be batch friendly\n",
    "            tgt = torch.cat((tgt, output), dim=1)\n",
    "\n",
    "        # add the lyrics\n",
    "        words = src[:, :, self.unembedded_input_size - 2].reshape(batch_size, seq_len, 1)\n",
    "        sylls = src[:, :, self.unembedded_input_size - 1].reshape(batch_size, seq_len, 1)        \n",
    "        tgt = torch.cat((tgt, words), dim=2)\n",
    "        tgt = torch.cat((tgt, sylls), dim=2)\n",
    "\n",
    "        return tgt\n",
    "    \n",
    "    # returns X but with the words and syllables embedded and positional encoded\n",
    "    def embed_lyrics(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        seq_len = X.shape[1]\n",
    "        assert X.shape[2] == self.unembedded_input_size\n",
    "\n",
    "        # extract words and syllables from X\n",
    "        words = X[:, :, self.unembedded_input_size - 2].long()\n",
    "        sylls = X[:, :, self.unembedded_input_size - 1].long()\n",
    "\n",
    "        # do embedding\n",
    "        words_embedded = self.word_embedding(words)\n",
    "        sylls_embedded = self.syll_embedding(sylls)\n",
    "\n",
    "        # reshape so you can concate\n",
    "        words_embedded = words_embedded.view(batch_size, seq_len, self.embed_size)\n",
    "        sylls_embedded = sylls_embedded.view(batch_size, seq_len, self.embed_size)\n",
    "\n",
    "        # concat everything and return\n",
    "        X_embedded = X[:, :, :self.unembedded_input_size - 2] # minus 2 to kill words and syllables\n",
    "        X_embedded = torch.cat((X_embedded, words_embedded), dim=2)\n",
    "        X_embedded = torch.cat((X_embedded, sylls_embedded), dim=2)\n",
    "\n",
    "        return X_embedded\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.word_embedding.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.syll_embedding.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.encoder_out.bias)\n",
    "        nn.init.uniform_(self.encoder_out.weight, -initrange, initrange)\n",
    "\n",
    "\n",
    "def discretize(sample):\n",
    "    authorized_values_pitch = torch.tensor(range(127))\n",
    "    authorized_values_duration = torch.tensor([0.25,  0.5, 0.75, 1., 1.5, 2., 3., 4., 6., 8., 16., 32.])\n",
    "    authorized_values_rest = torch.tensor([0., 1., 2., 4., 8., 16., 32.])\n",
    "\n",
    "    discretized = torch.zeros_like(sample)\n",
    "    # copy over words and syllables\n",
    "    discretized[:, 3] = sample[:, 3]\n",
    "    discretized[:, 4] = sample[:, 4]\n",
    "\n",
    "    for i in range(len(sample)): # for each note\n",
    "        # each finds the authorized value that has the least squared distance \n",
    "\n",
    "        # fix the pitch\n",
    "        best_idx = torch.argmin((sample[i][0] - authorized_values_pitch) ** 2)\n",
    "        discretized[i][0] = authorized_values_pitch[best_idx]\n",
    "\n",
    "        # fix the duration\n",
    "        best_idx = torch.argmin((sample[i][1] - authorized_values_duration) ** 2)\n",
    "        discretized[i][1] = authorized_values_duration[best_idx]\n",
    "        \n",
    "        # fix the rest\n",
    "        best_idx = torch.argmin((sample[i][2] - authorized_values_rest) ** 2)\n",
    "        discretized[i][2] = authorized_values_rest[best_idx]\n",
    "\n",
    "    return discretized\n",
    "\n",
    "def midi_to_note(midi: int, duration: float, syll: str):\n",
    "    duration = music21.duration.Duration(duration)\n",
    "\n",
    "    NOTES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    OCTAVES = list(range(11))\n",
    "    NOTES_IN_OCTAVE = len(NOTES)\n",
    "    \n",
    "    octave = midi // NOTES_IN_OCTAVE\n",
    "    note = NOTES[midi % NOTES_IN_OCTAVE]\n",
    "    note = music21.note.Note(f\"{note}{octave}\", duration = duration)\n",
    "    note.lyric = syll\n",
    "\n",
    "    return note\n",
    "\n",
    "def playSong(example):\n",
    "    syll_vocab = load('./vocabs/syll_vocab.npy')\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "    piano = pretty_midi.Instrument(program=piano_program)\n",
    "    pm.instruments.append(piano)\n",
    "    music21_notes = []\n",
    "\n",
    "    # add notes\n",
    "    time = 0\n",
    "    for note in example:\n",
    "        pitch = note[0].long().item()\n",
    "        dur = note[1].item()\n",
    "        rest_dur = note[2].item()\n",
    "        syll = syll_vocab[note[4].long().item()]\n",
    "\n",
    "        note = pretty_midi.Note(velocity=100, pitch = pitch, start = time + rest_dur, end = time + dur)\n",
    "        time = rest_dur + time + dur\n",
    "        piano.notes.append(note)\n",
    "\n",
    "        if (rest_dur != 0): music21_notes.append(music21.note.Rest(duration=music21.duration.Duration(rest_dur)))\n",
    "        \n",
    "        music21_notes.append(midi_to_note(pitch, dur, syll))\n",
    "\n",
    "\n",
    "    # synthesize audio\n",
    "    fs = 44100\n",
    "    audio_data = pm.fluidsynth(fs)\n",
    "    audio_data\n",
    "\n",
    "    return IPython.display.Audio(audio_data, rate=fs), audio_data, music21_notes\n",
    "\n",
    "def create_syll(words):\n",
    "    sylls = []\n",
    "    new_words = []\n",
    "    dic = pyphen.Pyphen(lang='nl_NL')\n",
    "\n",
    "    for word in words:\n",
    "        temp_syll = dic.inserted(word).split(\"-\")\n",
    "        temp_words = [word] * len(temp_syll)\n",
    "        sylls.append(temp_syll)\n",
    "        new_words.append(temp_words)\n",
    "    assert len(sylls) == len(new_words), \"input syllables size does not match words size\"\n",
    "    yield sylls\n",
    "    yield new_words\n",
    "\n",
    "def embed_lyrics(sylls, words):\n",
    "    word_vocab = load('./vocabs/word_vocab.npy')\n",
    "    syll_vocab = load('./vocabs/syll_vocab.npy')\n",
    "\n",
    "    for i in range(len(sylls)):\n",
    "        try:\n",
    "            sylls[i] = syll_vocab.where(a == sylls[i])[0][0]\n",
    "        except:\n",
    "            sylls[i] = random.randint(0, len(syll_vocab))\n",
    "        try:\n",
    "            words[i] = syll_vocab.where(a == words[i])[0][0]\n",
    "        except:\n",
    "            words[i] = random.randint(0, len(word_vocab))\n",
    "\n",
    "    yield sylls\n",
    "    yield words\n",
    "\n",
    "\n",
    "def create_song(lyrics):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    input = create_syll(lyrics)\n",
    "    syl_word = embed_lyrics(next(input), next(input))\n",
    "    sylls= next(syl_word)\n",
    "    words = next(syl_word)\n",
    "\n",
    "    seq_len = len(sylls)\n",
    "    batch_size = 1\n",
    "\n",
    "    gen = torch.load(\"GenModel.pt\", map_location=torch.device('cpu'))\n",
    "    gen = gen.to(device) \n",
    "\n",
    "    noise = torch.normal(0, 1, size=(batch_size ,seq_len, 4))\n",
    "    words = np.array(words).reshape(batch_size, seq_len, 1)\n",
    "    sylls = np.array(sylls).reshape(batch_size, seq_len, 1)\n",
    "    src = torch.cat((noise, torch.tensor(words)), dim=2)\n",
    "    src = torch.cat((src, torch.tensor(sylls)), dim=2)\n",
    "\n",
    "    tgt = gen(src)\n",
    "    generated_sample = tgt[0]\n",
    "    generated_sample = discretize(generated_sample)\n",
    "    mp3, audio_data, music21_notes = playSong(generated_sample)\n",
    "    soundfile.write('audio2.mp3', audio_data, 44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fluidsynth' has no attribute 'Synth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m create_song(\u001b[39m\"\u001b[39;49m\u001b[39mthis is a test\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[52], line 276\u001b[0m, in \u001b[0;36mcreate_song\u001b[1;34m(lyrics)\u001b[0m\n\u001b[0;32m    274\u001b[0m generated_sample \u001b[39m=\u001b[39m tgt[\u001b[39m0\u001b[39m]\n\u001b[0;32m    275\u001b[0m generated_sample \u001b[39m=\u001b[39m discretize(generated_sample)\n\u001b[1;32m--> 276\u001b[0m mp3, audio_data, music21_notes \u001b[39m=\u001b[39m playSong(generated_sample)\n\u001b[0;32m    277\u001b[0m soundfile\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39maudio2.mp3\u001b[39m\u001b[39m'\u001b[39m, audio_data, \u001b[39m44100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[52], line 214\u001b[0m, in \u001b[0;36mplaySong\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39m# synthesize audio\u001b[39;00m\n\u001b[0;32m    213\u001b[0m fs \u001b[39m=\u001b[39m \u001b[39m44100\u001b[39m\n\u001b[1;32m--> 214\u001b[0m audio_data \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39;49mfluidsynth(fs)\n\u001b[0;32m    215\u001b[0m audio_data\n\u001b[0;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m IPython\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mAudio(audio_data, rate\u001b[39m=\u001b[39mfs), audio_data, music21_notes\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pretty_midi\\pretty_midi.py:974\u001b[0m, in \u001b[0;36mPrettyMIDI.fluidsynth\u001b[1;34m(self, fs, sf2_path)\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m    973\u001b[0m \u001b[39m# Get synthesized waveform for each instrument\u001b[39;00m\n\u001b[1;32m--> 974\u001b[0m waveforms \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39mfluidsynth(fs\u001b[39m=\u001b[39mfs,\n\u001b[0;32m    975\u001b[0m                           sf2_path\u001b[39m=\u001b[39msf2_path) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruments]\n\u001b[0;32m    976\u001b[0m \u001b[39m# Allocate output waveform, with #sample = max length of all waveforms\u001b[39;00m\n\u001b[0;32m    977\u001b[0m synthesized \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(np\u001b[39m.\u001b[39mmax([w\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m waveforms]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pretty_midi\\pretty_midi.py:974\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m    973\u001b[0m \u001b[39m# Get synthesized waveform for each instrument\u001b[39;00m\n\u001b[1;32m--> 974\u001b[0m waveforms \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39;49mfluidsynth(fs\u001b[39m=\u001b[39;49mfs,\n\u001b[0;32m    975\u001b[0m                           sf2_path\u001b[39m=\u001b[39;49msf2_path) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruments]\n\u001b[0;32m    976\u001b[0m \u001b[39m# Allocate output waveform, with #sample = max length of all waveforms\u001b[39;00m\n\u001b[0;32m    977\u001b[0m synthesized \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(np\u001b[39m.\u001b[39mmax([w\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m waveforms]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pretty_midi\\instrument.py:468\u001b[0m, in \u001b[0;36mInstrument.fluidsynth\u001b[1;34m(self, fs, sf2_path)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m    467\u001b[0m \u001b[39m# Create fluidsynth instance\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m fl \u001b[39m=\u001b[39m fluidsynth\u001b[39m.\u001b[39;49mSynth(samplerate\u001b[39m=\u001b[39mfs)\n\u001b[0;32m    469\u001b[0m \u001b[39m# Load in the soundfont\u001b[39;00m\n\u001b[0;32m    470\u001b[0m sfid \u001b[39m=\u001b[39m fl\u001b[39m.\u001b[39msfload(sf2_path)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'fluidsynth' has no attribute 'Synth'"
     ]
    }
   ],
   "source": [
    "create_song(\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
